# -*- coding: utf-8 -*-
"""
Created on Wen Nov 08 15:25:50 2017

This script uses the connectome-convolutional neural network to classify instances 
in the public dataset accoring to age category based on resting-state functional 
connectivity matrices. All the weights and bias terms of the network are 
constants corresponding to the values learned previously on the in-house dataset.
These weights and bias terms were generated by the script 'ccnn_class_inhousetrain.py'
and are stored in 'weights_inhouse.pickle'. The results are saved into 
'results_ccnn_class_backtransfer.npz'.

This script was used for the condition 'backtransfer' in the paper
'Transfer learning improves resting-state functional connectivity pattern 
analysis using convolutional neural networks' by Vakli, Deák-Meszlényi, Hermann,
& Vidnyánszky.

This script is partially based on code from Deep learning course by Udacity: 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb

@author: Pál Vakli & Regina J. Deák-Meszlényi (RCNS-HAS-BIC)
"""
# %% ########################### Loading data #################################

# Importing necessary libraries
import numpy as np
import tensorflow as tf
from six.moves import cPickle as pickle

# loading the correlation matrices
picklefile = "CORR_tensor_public.pickle"
    
with open(picklefile, 'rb') as f:
    save = pickle.load(f)
    data_tensor = save['data_tensor']
    del save

data_tensor = data_tensor.astype(np.float32)

# Loading labels
labels_csv = np.loadtxt("labels_public.csv", delimiter=',')                                       
labels = labels_csv[:, 2]

# Loading weights
picklefile = "weights_inhouse.pickle"
with open(picklefile, 'rb') as f:
    save = pickle.load(f)
    layer1_weights_age = save['layer1_weights']
    layer1_biases_age = save['layer1_biases']
    layer2_weights_age = save['layer2_weights']
    layer2_biases_age = save['layer2_biases']
    layer3_weights_age = save['layer3_weights']
    layer3_biases_age = save['layer3_biases']
    layer4_weights_age = save['layer4_weights']
    layer4_biases_age = save['layer4_biases']
    del save    

# %% ####################### Function definitions #############################
# Define functions for tensor randomization, normalization, and performance 
# calculation

# normalize_tensor standardizes an n dimesional np.array to have zero mean and 
# standard deviation of 1
def normalize_tensor(data_tensor):
    data_tensor -= np.mean(data_tensor)
    data_tensor /= np.max(np.abs(data_tensor))
    return data_tensor

# randomize_tensor generates a random permutation of instances and the 
# corresponding labels before training
# INPUT: dataset: 4D tensor (np.array), instances are concatenated along the 
#                 first (0.) dimension
#        labels: 2D tensor (np.array), storing labels of instances in dataset,
#                 instances are concatenated along the first (0.) dimension, 
#                 number of columns corresponds to the number of classes, i.e. 
#                 labels are stored in one-hot encoding 
# OUTPUT: shuffled_dataset: 4D tensor (np.array), instances are permuted along 
#                           the first (0.) dimension
#         shuffled_labels: 2D tensor (np.array), storing labels of instances in 
#                           shuffled_dataset
def randomize_tensor(dataset, labels):                                         # sorrendcsere
    permutation = np.random.permutation(labels.shape[0])
    shuffled_dataset = dataset[permutation,:,:,:]
    shuffled_labels = labels[permutation,:]
    return shuffled_dataset, shuffled_labels

# accuracy calculates classification accuracy from one-hot encoded labels and 
# predictions
# INPUT: predictions: 2D tensor (np.array), storing predicted labels 
#                     (calculated with soft-max in our case) of instances with 
#                     one-hot encoding  
#       labels: 2D tensor (np.array), storing actual labels with one-hot 
#               encoding
# OUTPUT: accuracy in %
def accuracy(predictions, labels):
    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
            / predictions.shape[0])
  
# %% ####### Preparing the data and initializing network parameters ###########

numROI = 111
num_channels = 1
num_labels = 2
image_size = numROI
batch_size = 4
patch_size = image_size
keep_pr = 0.6   # the probability that each element is kept during dropout

# Replacing NaNs with 0s and normalizing data
data_tensor[np.isnan(data_tensor)] = 0
test_data = normalize_tensor(data_tensor)

# One-hot encoded train labels
num_labels = len(np.unique(labels))
test_labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)

# %% ###################### launching TensorFlow ##############################
  
# Drawing the computational graph  
graph = tf.Graph()
    
with graph.as_default():
    
    # Input data placeholders and constants
    tf_test_dataset = tf.constant(test_data)
      
    # Network weight and bias variables: Xavier initialization for better 
    # convergence in deep layers
    layer1_weights = tf.constant(layer1_weights_age, name="layer1_weights")
    layer1_biases = tf.constant(layer1_biases_age, name="layer1_biases")
    layer2_weights = tf.constant(layer2_weights_age, name="layer2_weights")
    layer2_biases = tf.constant(layer2_biases_age, name="layer2_biases")
    layer3_weights = tf.constant(layer3_weights_age, name="layer3_weights")
    layer3_biases = tf.constant(layer3_biases_age, name="layer3_biases")
    layer4_weights = tf.constant(layer4_weights_age, name="layer4_weights")
    layer4_biases = tf.constant(layer4_biases_age, name="layer4_biases")
        
    # Convolutional network architecture
    def model(data, keep_pr):
        # First layer: line-by-line convolution with ReLU and dropout
        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')
        hidden = tf.nn.dropout(tf.nn.relu(conv+layer1_biases), keep_pr)
        # Second layer: convolution by column with ReLU and dropout
        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='VALID')
        hidden = tf.nn.dropout(tf.nn.relu(conv+layer2_biases), keep_pr)
        # Third layer: fully connected hidden layer with dropout and ReLU
        shape = hidden.get_shape().as_list()
        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
        hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), keep_pr)
        # Fourth (output) layer: fully connected layer with logits as output
        return tf.matmul(hidden, layer4_weights) + layer4_biases
      
    # Calculate predictions from test data (keep_pr of dropout is 1!)
    test_prediction = tf.nn.softmax(model(tf_test_dataset, 1))
    
# Start TensorFlow session
with tf.Session(graph=graph) as session:
    
    # Initializing variables    
    tf.global_variables_initializer().run()
    print('\nVariables initialized ...')
        
    # Evaluate the trained model on the test data in the given fold
    test_pred = test_prediction.eval()

# Calculate final accuracy    
print('\nOverall test accuracy: %.1f%%' % accuracy(test_pred, test_labels))
    
# Saving data
np.savez("results_ccnn_class_backtransfer.npz", \
    labels=test_labels, predictions=test_pred)