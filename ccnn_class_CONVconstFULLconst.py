# -*- coding: utf-8 -*-
"""
Created on Thu Oct 19 14:59:51 2017

This script uses the connectome-convolutional neural network trained on the 
public dataset to classify resting-state functional connectivity matrices 
in the in-house dataset / NKI-RS subset according to age category. All the 
weights and bias terms of the network are constants corresponding to the values 
learned previously on the public dataset.These weights and bias terms were 
generated by the script 'ccnn_class_publictrain.py' and are stored in 
'weights_public.pickle'. The results are saved into 
'results_ccnn_class_CONVconstFULLconst_inhouse.npz' or
'results_ccnn_class_CONVconstFULLconst_NKI-RS_subset.npz'.

This script was used for the condition 'CONVconstFULLconst' in the manuscript
'Transfer learning improves resting-state functional connectivity pattern 
analysis using convolutional neural networks' by Vakli, Deák-Meszlényi, Hermann,
& Vidnyánszky.

This script is partially based on code from Deep learning course by Udacity: 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb

@author: Pál Vakli & Regina J. Deák-Meszlényi (RCNS-HAS-BIC)
"""
# %% #################### Choosing the target dataset #########################
# To use the in-house dataset as the target dataset, you have to run this script
# with 'target_data' set to 1. To use the NKI-RS subset as the target dataset, 
# you have to set 'target_data' to 2. 
target_data = 1   # 1 = in-house dataset
                  # 2 = NKI-RS subset
             
# %% ########################### Loading data #################################

# Importing necessary libraries
import numpy as np
import tensorflow as tf
from six.moves import cPickle as pickle

# loading the correlation matrices
if target_data == 1:
    picklefile = "CORR_tensor_inhouse.pickle"
elif target_data == 2:
    picklefile = "CORR_tensor_NKI-RS_subset.pickle"

with open(picklefile, 'rb') as f:
    save = pickle.load(f)
    data_tensor = save['data_tensor']
    del save

data_tensor = data_tensor.astype(np.float32)

# Loading labels
if target_data == 1:
    labels_csv = np.loadtxt("labels_inhouse.txt", delimiter=',')                              
elif target_data == 2:
    labels_csv = np.loadtxt("labels_NKI-RS_subset.csv", delimiter=',')
labels = labels_csv[:, 1]

# Loading weights
picklefile = "weights_public.pickle"
with open(picklefile, 'rb') as f:
    save = pickle.load(f)
    layer1_weights_age = save['layer1_weights']
    layer1_biases_age = save['layer1_biases']
    layer2_weights_age = save['layer2_weights']
    layer2_biases_age = save['layer2_biases']
    layer3_weights_age = save['layer3_weights']
    layer3_biases_age = save['layer3_biases']
    layer4_weights_age = save['layer4_weights']
    layer4_biases_age = save['layer4_biases']
    del save    

# %% ####################### Function definitions #############################
# Define functions for tensor randomization, normalization, and performance 
# calculation

# normalize_tensor standardizes an n dimesional np.array to have zero mean and 
# standard deviation of 1
def normalize_tensor(data_tensor):
    data_tensor -= np.mean(data_tensor)
    data_tensor /= np.max(np.abs(data_tensor))
    return data_tensor

# randomize_tensor generates a random permutation of instances and the 
# corresponding labels before training
# INPUT: dataset: 4D tensor (np.array), instances are concatenated along the 
#                 first (0.) dimension
#        labels: 2D tensor (np.array), storing labels of instances in dataset,
#                 instances are concatenated along the first (0.) dimension, 
#                 number of columns corresponds to the number of classes, i.e. 
#                 labels are stored in one-hot encoding 
# OUTPUT: shuffled_dataset: 4D tensor (np.array), instances are permuted along 
#                           the first (0.) dimension
#         shuffled_labels: 2D tensor (np.array), storing labels of instances in 
#                           shuffled_dataset
def randomize_tensor(dataset, labels):
    permutation = np.random.permutation(labels.shape[0])
    shuffled_dataset = dataset[permutation,:,:,:]
    shuffled_labels = labels[permutation,:]
    return shuffled_dataset, shuffled_labels

# accuracy calculates classification accuracy from one-hot encoded labels and 
# predictions
# INPUT: predictions: 2D tensor (np.array), storing predicted labels 
#                     (calculated with soft-max in our case) of instances with 
#                     one-hot encoding  
#       labels: 2D tensor (np.array), storing actual labels with one-hot 
#               encoding
# OUTPUT: accuracy in %
def accuracy(predictions, labels):
    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
            / predictions.shape[0])
  
# %% ###### Preparing the test set and initializing network parameters ########

keep_pr = 0.6 # the probability that each element is kept during dropout

# Replacing NaNs with 0s and normalizing test data
data_tensor[np.isnan(data_tensor)] = 0
test_data = normalize_tensor(data_tensor)

# One-hot encoded test labels
num_labels = len(np.unique(labels))
test_labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)

# %% ####################### launching TensorFlow #############################

# Drawing the computational graph    
graph = tf.Graph()
    
with graph.as_default():
    
    # Input data placeholders and constants
    tf_test_dataset = tf.constant(test_data)
      
    # Weights and baises are constants
    layer1_weights = tf.constant(layer1_weights_age, name="layer1_weights")
    layer1_biases = tf.constant(layer1_biases_age, name="layer1_biases")
    layer2_weights = tf.constant(layer2_weights_age, name="layer2_weights")
    layer2_biases = tf.constant(layer2_biases_age, name="layer2_biases")
    layer3_weights = tf.constant(layer3_weights_age, name="layer3_weights")
    layer3_biases = tf.constant(layer3_biases_age, name="layer3_biases")
    layer4_weights = tf.constant(layer4_weights_age, name="layer4_weights")
    layer4_biases = tf.constant(layer4_biases_age, name="layer4_biases")
        
    # Convolutional network architecture
    def model(data, keep_pr):
        # First layer: line-by-line convolution with ReLU and dropout
        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')
        hidden = tf.nn.dropout(tf.nn.relu(conv+layer1_biases), keep_pr)
        # Second layer: convolution by column with ReLU and dropout
        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='VALID')
        hidden = tf.nn.dropout(tf.nn.relu(conv+layer2_biases), keep_pr)
        # Third layer: fully connected hidden layer with dropout and ReLU
        shape = hidden.get_shape().as_list()
        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])
        hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases), keep_pr)
        # Fourth (output) layer: fully connected layer with logits as output
        return tf.matmul(hidden, layer4_weights) + layer4_biases
      
    # Calculate predictions from test data (keep_pr of dropout is 1!)
    test_prediction = tf.nn.softmax(model(tf_test_dataset, 1))
    
# Start TensorFlow session
with tf.Session(graph=graph) as session:
    
    # Initializing variables     
    tf.global_variables_initializer().run()
    print('\nVariables initialized ...')
    
    # Calculating test predictions    
    test_pred = test_prediction.eval()

# Calculate final accuracy    
print('\nOverall test accuracy: %.1f%%' % accuracy(test_pred, test_labels))
    
# Saving results
if target_data == 1:
    np.savez("results_ccnn_class_CONVconstFULLconst_inhouse.npz", \
        labels=test_labels, predictions=test_pred)
elif target_data == 2:
    np.savez("results_ccnn_class_CONVconstFULLconst_NKI-RS_subset.npz", \
        labels=test_labels, predictions=test_pred)